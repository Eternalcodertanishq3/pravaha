# PravÄha â€” à¤ªà¥à¤°à¤µà¤¾à¤¹

**A vLLM-inspired LLM inference engine with continuous batching and PagedAttention.**

PravÄha means "flow/stream" in Sanskrit, symbolizing continuous batching and token streaming.

## Architecture

```mermaid
graph TD
    User([User Request]) --> Engine
    subgraph Pravaha [PravÄha Engine]
        Engine[Engine Orchestrator] --> Loader[Model Loader]
        Engine --> Decoder[Autoregressive Decoder]
        Decoder -->|Token Generation| Model(Transformer Model)
        Decoder <-->|State Management| KVCache[Naive KV-Cache]
        KVCache -->|Prefill/Update| Model
    end
    Decoder --> Output([Streaming Token Output])
    style KVCache fill:#f9f,stroke:#333,stroke-width:2px,color:#000
```

## Features (Phase 1 & 2 Completed)

- âœ… **HuggingFace Model Loading**: Support for GPT-2, Llama, Mistral with configurable device mapping.
- âœ… **Naive KV-Cache (Phase 2)**: Custom Python-based Key-Value cache for deterministic memory usage and zero fragmentation.
- âœ… **Streaming Generation**: Low-latency token streaming (<10ms).
- âœ… **Configurable Dtype**: FP16/BF16/FP32 support.
- âœ… **Sampling Pipeline**: Temperature, Top-K, Top-P, Repetition Penalty.

## ðŸŽ¥ Demos

**Phase 1: Foundation** (Baseline Inference)
https://github.com/user-attachments/assets/32ba41bb-b0ea-45ff-b167-ae13927faeaf

**Phase 2: Acceleration** (Naive KV-Cache + Streaming)
https://github.com/user-attachments/assets/ea9071da-2285-4f15-a385-c551eada8882

## Roadmap

- âœ… **Phase 1: Foundation (Loader & Inference)**
- âœ… **Phase 2: Naive KV-Cache + Streaming Generation**
  - _Implemented_: A custom, pre-allocated KV-cache that provides 100% visibility into memory usage (e.g., 36MB for GPT-2). This replaces the opaque HuggingFace cache, giving us full control over state management.
- âœ… **Phase 3: Continuous Batching Scheduler**
  - _Implemented_: A slot-based dynamic scheduler operating in a background thread with an `asyncio` frontend. It handles disjoint batched prefill and batched decode passes for multiple concurrent users.
- ðŸ”² **Phase 4: Paged KV-Cache + BlockAllocator**
- ðŸ”² **Phase 5: INT8/INT4 Quantization (GPTQ/AWQ)**
- ðŸ”² Phase 6: API Server + Streaming
- ðŸ”² Phase 7: Metrics + Profiler
- ðŸ”² Phase 8: FlashAttention + Speculative Decoding

## Quick Start

```bash
# Install
pip install -e ".[dev]"

# Run
python -c "
from pravaha.engine import PravahaEngine
engine = PravahaEngine()
for token in engine.generate('Once upon a time', max_new_tokens=50, temperature=0.8):
    print(token, end='', flush=True)
print()
"

# Tests
python -m pytest tests/ -v             # Fast tests only
python -m pytest tests/ -v --run-slow  # All tests (downloads models)
```

## Project Structure

```
pravaha/
â”œâ”€â”€ config.py           # Pydantic configuration system
â”œâ”€â”€ engine.py           # Top-level inference orchestrator
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ loader.py       # HuggingFace model loading
â”‚   â”œâ”€â”€ model_config.py # Architecture detection
â”‚   â””â”€â”€ weights.py      # Weight loading utilities
â”œâ”€â”€ tokenizer/
â”‚   â””â”€â”€ tokenizer.py    # HuggingFace tokenizer wrapper
â”œâ”€â”€ decoder/
â”‚   â”œâ”€â”€ decoder.py      # Autoregressive decode loop
â”‚   â””â”€â”€ sampling.py     # Sampling strategies
â”œâ”€â”€ scheduler/
â”‚   â””â”€â”€ request.py      # Request/sequence data structures
â”œâ”€â”€ kv_cache/           # (Phase 2-4)
â”œâ”€â”€ quantization/       # (Phase 5)
â”œâ”€â”€ server/             # (Phase 6)
â””â”€â”€ metrics/            # (Phase 7)
```

## License

MIT
