# PravÄha â€” à¤ªà¥à¤°à¤µà¤¾à¤¹

**A vLLM-inspired LLM inference engine with continuous batching and PagedAttention.**

PravÄha means "flow/stream" in Sanskrit, symbolizing continuous batching and token streaming.

## Architecture

```mermaid
graph TD
    User([User Request]) --> Engine
    subgraph Pravaha [PravÄha Engine]
        Engine[Engine Orchestrator] --> Loader[Model Loader]
        Engine --> Decoder[Autoregressive Decoder]
        Decoder -->|Token Generation| Model(Transformer Model)
        Decoder <-->|State Management| KVCache[Naive KV-Cache]
        KVCache -->|Prefill/Update| Model
    end
    Decoder --> Output([Streaming Token Output])
    style KVCache fill:#f9f,stroke:#333,stroke-width:2px,color:#000
```

## âœ¨ Features (Phases 1-3 Completed)

- âœ… **Continuous Batching Scheduler (Phase 3)**: A dynamic, slot-based execution engine that maximizes GPU utilization by grouping incoming requests in an asynchronous `asyncio` background loop. Features disjoint phase execution for efficient batched prefill and decoding passes without mixed-task kernel complexity.
- âœ… **Custom KV-Cache Management (Phase 2)**: 100% Python-based pre-allocated Key-Value cache for multi-layered transformer blocks. Provides precise visibility into memory allocation (e.g., exactly 144MB for a 4-slot GPT-2 cache) and fully replaces opaque native HF caching.
- âœ… **HuggingFace Native Interoperability (Phase 1)**: Zero-friction model loading for state-of-the-art architectures (GPT-2, Llama, Mistral) through dynamic state conversion, bringing advanced batching to standard huggingface checkpoints without kernel modification.
- âœ… **High-Performance Streaming Generation**: Fully unblocked end-to-end token streaming driven by decoupled background threads queueing natively to `asyncio` event loops.
- âœ… **Precision Controls**: Configurable FP16, BF16, and FP32 torch datatypes natively managed at loop initiation.
- âœ… **Configurable Sampling Pipeline**: Robust generation controls including Temperature, Top-K, Top-P stochastic sampling, and custom stop-word parameters.

## ðŸ“ˆ Roadmap & Technical Achievements

- âœ… **Phase 1: Foundation (Loader & Engine Scaffold)**
  - _Goal:_ Create a lightweight inference orchestrator.
  - _Achievement:_ Implemented the base `PravahaEngine`, decoupling tokenization and model weights while proving the feasibility of streaming inference.

- âœ… **Phase 2: Acceleration (Naive KV-Cache + Deterministic State)**
  - _Goal:_ Strip control from HuggingFace auto-regressive generation loops.
  - _Achievement:_ Built a custom `NaiveKVCache` that pre-allocates exactly the tensors required for continuous inference instead of constantly resizing memory arrays. This removed the opaque HF caching and unlocked the data structures fundamentally required for concurrent multi-sequence scheduling.

- âœ… **Phase 3: Continuous Batching Scheduler**
  - _Goal:_ Substantially increase hardware throughput via concurrent inference.
  - _Achievement:_ Designed an asynchronous frontend wrapped around a synchronous PyTorch thread-loop. The `ContinuousScheduler` handles concurrent inputs by employing **Disjoint Execution Phases**. It waits to batch un-allocated requests together for an isolated _Batched Prefill Pass_, and then cleanly executes multi-sequence _Batched Decode Passes_, dynamically hiding single-batch latency to near zero (e.g., executing 4 concurrent GPT-2 blocks in just 1.10 seconds total).

- ðŸ”² **Phase 4: Paged KV-Cache + BlockAllocator**
- ðŸ”² **Phase 5: INT8/INT4 Quantization (GPTQ/AWQ)**
- ðŸ”² Phase 6: API Server + FastAPI Streaming
- ðŸ”² Phase 7: Real-time Telemetry & Profiling
- ðŸ”² Phase 8: FlashAttention & Speculative Decoding Integration

## ðŸŽ¥ Demos

**Phase 1: Foundation** (Baseline Inference)
https://github.com/user-attachments/assets/32ba41bb-b0ea-45ff-b167-ae13927faeaf

**Phase 2: Acceleration** (Naive KV-Cache + Streaming)
https://github.com/user-attachments/assets/ea9071da-2285-4f15-a385-c551eada8882

**Phase 3: Continuous Batching** (Dynamic Slot Allocation)
_(Recording Pending)_

## Quick Start

```bash
# Install
pip install -e ".[dev]"

# Run
python -c "
from pravaha.engine import PravahaEngine
engine = PravahaEngine()
for token in engine.generate('Once upon a time', max_new_tokens=50, temperature=0.8):
    print(token, end='', flush=True)
print()
"

# Tests
python -m pytest tests/ -v             # Fast tests only
python -m pytest tests/ -v --run-slow  # All tests (downloads models)
```

## Project Structure

```
pravaha/
â”œâ”€â”€ config.py           # Pydantic configuration system
â”œâ”€â”€ engine.py           # Top-level inference orchestrator
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ loader.py       # HuggingFace model loading
â”‚   â”œâ”€â”€ model_config.py # Architecture detection
â”‚   â””â”€â”€ weights.py      # Weight loading utilities
â”œâ”€â”€ tokenizer/
â”‚   â””â”€â”€ tokenizer.py    # HuggingFace tokenizer wrapper
â”œâ”€â”€ decoder/
â”‚   â”œâ”€â”€ decoder.py      # Autoregressive decode loop
â”‚   â””â”€â”€ sampling.py     # Sampling strategies
â”œâ”€â”€ scheduler/
â”‚   â””â”€â”€ request.py      # Request/sequence data structures
â”œâ”€â”€ kv_cache/           # (Phase 2-4)
â”œâ”€â”€ quantization/       # (Phase 5)
â”œâ”€â”€ server/             # (Phase 6)
â””â”€â”€ metrics/            # (Phase 7)
```

## License

MIT
